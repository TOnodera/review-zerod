{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確率的勾配降下法\n",
    "$W \\gets nW$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確率的勾配降下法 (いままで使ってたやり方)\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.key():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum法\n",
    "\n",
    "$v \\gets av - nW$\n",
    "\n",
    "勾配が０に使づいてほとんど動かなくなってもaによって動く(-nWと逆の符号なのでブレーキ的な働きをする)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Momentum法\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zero_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdamGrad\n",
    "$$\n",
    "   g_t = \\nabla f(\\theta_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t \\odot g_t\n",
    "$$\n",
    "$$\n",
    "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "$$\n",
    "   \\theta_t = \\theta_{t-1} - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "前回の勾配のノルムが大きいほど更新時の値は小さくなる(vtのおかげで)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# AdamGrad\n",
    "class AdamGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zero_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            # アダマール積\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初期値の選定方法\n",
    "色々らしい。\n",
    "- 適当に０に近い値とか選ぶとどの層の分布も同じような値になったり、極端な値に偏ったりする。\n",
    "- 適切な初期値を与えるのは難しい問題らしい。\n",
    "- xavierの初期値という方法がある。\n",
    "\n",
    "## chatgptによるxavierの初期値の説明\n",
    "### Xavier Initialization の数理的背景\n",
    "\n",
    "Xavier 初期化は、ニューラルネットワークの学習を安定させるための重みの初期化手法です。この手法では、各層において重みが平均 0、特定の標準偏差を持つ正規分布または一様分布からサンプリングされます。\n",
    "\n",
    "---\n",
    "\n",
    "### 基本アイデア\n",
    "\n",
    "Xavier 初期化は、以下の2つを目的としています：\n",
    "1. **フォワードパスの出力が適切にスケールされること**：\n",
    "   - 各層での出力の分散が極端に大きくなったり小さくなったりしない。\n",
    "2. **バックプロパゲーションの勾配が適切にスケールされること**：\n",
    "   - 勾配爆発や勾配消失を防ぎ、学習を安定させる。\n",
    "\n",
    "---\n",
    "\n",
    "### 数式\n",
    "\n",
    "重み行列 \\(W\\) を次のように初期化します：\n",
    "\n",
    "### 1. 正規分布の場合\n",
    "\\[\n",
    "W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
    "\\]\n",
    "\n",
    "- 平均：\\(0\\)\n",
    "- 標準偏差：\\(\\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}}\\)\n",
    "\n",
    "### 2. 一様分布の場合\n",
    "\\[\n",
    "W \\sim U\\left(-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
    "\\]\n",
    "\n",
    "- 範囲：\\(\\pm \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\)\n",
    "\n",
    "ここで：\n",
    "- \\(n_{\\text{in}}\\)：現在の層への入力ユニット数\n",
    "- \\(n_{\\text{out}}\\)：現在の層からの出力ユニット数\n",
    "\n",
    "---\n",
    "\n",
    "### 例：入力層から隠れ層への初期化\n",
    "\n",
    "仮に以下の設定があるとします：\n",
    "- 入力ユニット数：\\(n_{\\text{in}} = 784\\)（MNISTの画像ピクセル数）\n",
    "- 出力ユニット数：\\(n_{\\text{out}} = 128\\)（隠れ層のニューロン数）\n",
    "\n",
    "### 正規分布の場合\n",
    "標準偏差は次のように計算されます：\n",
    "\\[\n",
    "\\sigma = \\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}} = \\sqrt{\\frac{2}{784 + 128}} = \\sqrt{\\frac{2}{912}} \\approx 0.0469\n",
    "\\]\n",
    "\n",
    "重みは次の分布に従います：\n",
    "\\[\n",
    "W \\sim \\mathcal{N}\\left(0, 0.0469\\right)\n",
    "\\]\n",
    "\n",
    "### 一様分布の場合\n",
    "範囲は次のように計算されます：\n",
    "\\\n",
    "\\pm \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}} = \\pm \\sqrt{\\frac{6}{784 + 128}} = \\pm \\sqrt{\\frac{6}{912}} \\approx \\pm 0.0814\n",
    "\\]\n",
    "\n",
    "重みは次の分布に従います：\n",
    "\\[\n",
    "W \\sim U\\left(-0.0814, 0.0814\\right)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Xavier 初期化の重要性\n",
    "\n",
    "1. **バランスの取れたスケール**：\n",
    "   - 各層での出力や勾配のスケールが均一に保たれます。\n",
    "2. **勾配消失・爆発の防止**：\n",
    "   - 重みの初期化が適切な範囲で行われるため、学習が安定します。\n",
    "3. **活性化関数との相性**：\n",
    "   - 活性化関数が非線形性を発揮しやすくなり、学習効率が向上します。\n",
    "\n",
    "---\n",
    "\n",
    "### 参考\n",
    "\n",
    "- Xavier 初期化は、活性化関数としてシグモイドや双曲線正接（tanh）を使用する場合に特に有効です。\n",
    "- ReLU の場合には、**He 初期化**がより適切な場合があります（\\(\\sqrt{\\frac{2}{n_{\\text{in}}}}\\) を使用）。\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "chatgptによるxavierの初期値の説明ここまで\n",
    "\n",
    "これの何がうれしいかというと、入力パラメータ数に関係なく正規分布でばらつくので偏りがなくなる。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
